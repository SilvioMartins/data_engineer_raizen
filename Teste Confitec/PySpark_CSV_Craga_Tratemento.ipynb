{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Bilbliotecas\n",
    "### Aqui Usaremos as libs spark necessárias para o desenvolviemnto. SparkSession, que permite a execução do código Spark ou no cluster ou standalone; Functions, dando acesso às várias funções e;  Types, pois trabalharemos com os tipos Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação da sessão Spark\n",
    "### Uso do sparkSession para criar o ponto de entrada do spark para se trabalhar com RDD´s, dataframes e Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Exemplo de ETL CSV\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-definindo o Schema \n",
    "### Uso do Schema pré-definido se torna Muito mais rápido do que deixar o spark fazer a inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = StructType() \\\n",
    "        .add(\"data\", StringType(),True) \\\n",
    "        .add(\"dia_semana\", StringType(),True) \\\n",
    "        .add(\"operacao\", StringType(),True) \\\n",
    "        .add(\"TMR\", StringType(),True) \\\n",
    "        .add(\"solicitacao_site\", StringType(),True) \\\n",
    "        .add(\"entregues_novos\", StringType(),True) \\\n",
    "        .add(\"perc_entregues\", StringType(),True) \\\n",
    "        .add(\"atendidos_novos\", StringType(),True) \\\n",
    "        .add(\"entregues_reaprov\", StringType(),True) \\\n",
    "        .add(\"atendidos_reaprov\", StringType(),True) \\\n",
    "        .add(\"perc_atend_nr\", StringType(),True) \\\n",
    "        .add(\"perc_atend_novos\", StringType(),True) \\\n",
    "        .add(\"perc_ete_novos\", StringType(),True) \\\n",
    "        .add(\"entregues_nr\", StringType(),True) \\\n",
    "        .add(\"atendidos_nr\", StringType(),True) \\\n",
    "        .add(\"perc_atend_total\", StringType(),True) \\\n",
    "        .add(\"aband_novos\", StringType(),True) \\\n",
    "        .add(\"aband_reaprov\", StringType(),True) \\\n",
    "        .add(\"cham_curta_novos\", StringType(),True) \\\n",
    "        .add(\"cham_curta_reaprov\", StringType(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura CSV\n",
    "### Leitura do CSV especificando o Format, com cabeçalho, codificaçao, sshema e delimitadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "     .option(\"header\", True) \\\n",
    "     .option(\"encoding\",\"UTF-8\") \\\n",
    "     .option(\"delimiter\",\";\") \\\n",
    "     .schema(sch) \\\n",
    "     .load(\"/content/drive/MyDrive/ESTUDOS/CSV/relatorio15.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "### Aqui faremos ajustes e limpezas no dataframe, como ajustes do campo data que vem em formato Brasileio TimeStamp, e retirada do caracter \"%\" de algumas colunas numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de uma UDF (Função Definida por Usuário) para tratamento dos campos data, mudando a coluna de timestamp para o format yyyy-MM-dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType()) \n",
    "def udt_trat(str):\n",
    "    return str[6:10]+'-'+str[3:5]+'-'+str[0:2]\n",
    "\n",
    "df = df.withColumn('data', udt_trat(col('data')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso do Regexp_replace para retirar o caracter \"%\" de algumas colunas. Pode-se usar também  o Translat criando um novo dataframe (newDf = testDF.withColumn('d_id', translate('d_id', 'a', '0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('perc_entregues', regexp_replace('perc_entregues','%',''))\n",
    "df = df.withColumn('perc_atend_nr', regexp_replace('perc_atend_nr','%',''))\n",
    "df = df.withColumn('perc_atend_novos', regexp_replace('perc_atend_novos','%',''))\n",
    "df = df.withColumn('perc_ete_novos', regexp_replace('perc_ete_novos','%',''))\n",
    "df = df.withColumn('perc_atend_total', regexp_replace('perc_atend_total','%',''))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
